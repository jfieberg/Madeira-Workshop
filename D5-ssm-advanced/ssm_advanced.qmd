---
title: "Building state-space models with RTMB"
format: html
editor: visual
---

<!-- To be able to have continuous line numbers -->

```{=html}
<style>
body
{ counter-reset: source-line 0; }
pre.numberSource code
{ counter-reset: none; }
</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Credits

This tutorial uses materials that was created for the Appendix of the paper Auger-Méthé M, Newman K, Cole D, Empacher F, Gryba R, King AA, Leos-Barajas V, Mills Flemming J, Nielsen A, Petris G, Thomas L (2021) A guide to state–space modeling of ecological time series. Ecological Monographs 91(4):e01470.  https://doi.org/10.1002/ecm.1470 Some of the material is taken verbatim. Please refer and cite the paper rather than this code if appropriate.

# Tutorial goals

The goal of this tutorial is to start exploring how one can write your one state-space model.

The primary learning objectives are to:

1.  Understand how to write a state-space model using `RTMB` through a simulation example.
2.  Write a simple state-space model for Argos movement data in `RTMB`.

# General setup

First, let's load the packages that we will need to complete the analyses. Of course you need to have them installed first.

```{r Load packages, attr.source = ".numberLines", message = FALSE, warning = FALSE}
library(tidyverse)  # data management
library(ggspatial)  # plot the data
library(sf)         # spatial data processing
library(here)       # To help with sourcing
library(RTMB)
```

# Introducing the method with a toy state-space model

## Model

We start with a simple state-space model (SSM). This model is not linked to an ecological example; it's just a teaching tool. 
It is a linear SSM with normal error distributions that consists of two equations for two time series.

The process equation is:
\begin{equation}
  z_t = z_{t-1} + \epsilon_t, \;\;\; \epsilon_t \sim \text{N}(0, \sigma_p^2),
  \label{E.toy2p.p} 
\end{equation}
where $z_t$ is the state value at time $t$, for $t=1, ..., T$. The states are generally unknown, i.e., cannot be observed directly, and are sometimes referred to as latent states. This equation represents the evolution of the hidden state as a random walk. The equation implies that there is some process variation and is here described by a Normal distribution with standard deviation $\sigma_p$. For simplicity, we set the initial state value to be 0, i.e., $z_0 =0$.

The observation equation is:
\begin{equation}
  y_t = z_{t} + \eta_t, \;\;\; \eta_t \sim \text{N}(0, \sigma_o^2),
  \label{E.toy2p.o}
\end{equation}
where $y_t$ is the observation at time $t$, for $t=1, ..., T$. This equation links the observation at time $t$ to the underlying state at that time. The equation implies that there are observation error and, here, is described by a Normal distribution with standard deviation $\sigma_o$.

This model has two parameters: $\sigma_o, \sigma_p$.

## Simulated data

To first explore how `RTMB` works, we will use data simulated from this model.

First, let us simulate the process for a time series of length 200 ($T=200$), with an additional time step for the state at $t=0$. To be consistent with the model description, we set to $z_0 = 0$. We choose the standard deviation of the process variation, $\sigma_p$, to be 0.1.

```{r process.sim, attr.source = ".numberLines", tidy=FALSE}
# Create a vector that will keep track of the states
# It's of length T + 1 (+1 for t=0)
# T is not a good name in R, because of T/F, so we use TT
TT <- 200
z <- numeric(TT + 1)
# Standard deviation of the process variation
sdp <- 0.1
# Set the seed, so we can reproduce the results
set.seed(553)
# For-loop that simulates the state through time, using i instead of t,
for(i in 1:TT){
  # This is the process equation
  z[i+1] <- z[i] + rnorm(1, 0, sdp)
  # Note that this index is shifted compared to equation in text,
  # because we assume the first value to be at time 0
}
```

Let us plot the time series we have created.

```{r process.sim.fig, attr.source = ".numberLines"}
plot(0:TT, z,
     pch = 19, cex = 0.7, col="red", ty = "o", 
     xlab = "t", ylab = expression(z[t]), las=1)
```

Second, let us simulate the observations. We set the standard deviation of the observation error $\sigma_o$ to 0.1.

```{r obs.sim, attr.source = ".numberLines", tidy=FALSE}
# Create a vector that will keep track of the observations
# It's of length T
y <- numeric(TT)
# Standard deviation of the observation error
sdo <- 0.1
# For t=1, ... T, add measurement error
# Remember that z[1] is t=0
y <- z[2:(TT+1)] + rnorm(TT, 0, sdo)
```

Let us plot both the observations and the states. From now on, we are adding extra space on the y-axis to leave space for the legend. Note that the space we assigned may not work for all figure sizes.

```{r obs.sim.fig, attr.source = ".numberLines", tidy=FALSE}
plot(1:TT, y,
     pch=3, cex = 0.8, col="blue", ty="o", lty = 3,
     xlab = "t", ylab = expression(z[t]),
     xlim = c(0,TT), ylim = c(min(y), max(y)+max(y)/5),
     las = 1)
points(0:TT, z,
       pch = 19, cex = 0.7, col="red", ty = "o")
legend("top",
       legend = c("Obs.", "True latent states"),
       pch = c(3, 19),
       col = c("blue", "red"),
       lty = c(3, 1),
       horiz=TRUE, bty="n", cex=0.9)
```

With real data, we usually only have the observations, $y_t$, and do not know the true states, $z_t$. However, simulated data allow us to see the discrepancies between the observed values and the values for the process they represent.

## Fitting the model using `RTMB`

In `RTMB` you need to create a function that computes the value of the negative log likelihood value for a given set of parameter values. Then, you use that function in one of R's optimizers to find the minimum negative log likelihood. The beauty of `TMB` is that it uses the Laplace approximation to integrate over the states and computes the gradient efficiently, which speeds up the optimizing process.

### Writing the negative log-likelihood function

Now, let us create the negative log-likelihood function that we will minimize (equivalent of maximizing the likelihood).

```{r toy2p, attr.source = ".numberLines", tidy=FALSE}
# Define main function
toy2p <- function(parms, data){
  # Allow to access items from these objects directly 
  getAll(dataTmb, parms, warn=FALSE)
  ## Optional (enables extra RTMB features)
  # Tells RTMB that it's the response
  y <- OBS(y)

  # Specify the parameters transformation
  # Transform standard deviations
  # exp(par) is a trick to make sure that the estimated sd > 0
  sdp <- exp(logSdP)
  sdo <- exp(logSdO)
  
  ## Initialize joint negative log likelihood
  nll <- 0.0

  # Calculate the contribution to the negative log-likelihood
  # of the process equation for t=1,...,T
  # Remember that we fixed z_0 = 0
  for(i in 2:length(z)){
    nll <- nll - dnorm(z[i], mean=z[i-1], sd=sdp, log=TRUE)
  }
  
  
   
  

  # Calculate the contribution to the negative log-likelihood
  # of the observation equation for t=1,...,T
  # Remember, the first element of z is at t=0,
  # while the first element of y is at t=1
  for(i in 1:length(y)){
    nll <- nll - dnorm(y[i], mean=z[i+1], sd=sdo, log=TRUE)
  }
  
   
    
  # State the transformed parameters to report
  # Using ADREPORT will return the point values and the standard errors
  # Note that we only need to specify this for parameters
  # we transformed, see section D above
  # The other parameters, including the random effects (states),
  # will be returned automatically
  ADREPORT(sdp)
  ADREPORT(sdo)

   #State that we want the negative log-likelihood to be returned
  #This is what the optimizer in R will minimize
  return(nll)
  
}
```

### Preparing the model for fitting

First, we need to prepare the data. This is going to be a list with all the data items needed in the function. It needs to have the same name as the one we defined in the function, here `dataTmb`. Here, we only have `y` which is the time series of observed values.

```{r prepY, attr.source = ".numberLines"}
dataTmb <- data.frame(y = y)
```

Second, we need a list with the parameters. Again the names need to match those in the function. These are the starting values for the parameters. Note that this includes both the parameters and the states.

```{r parPrep, attr.source = ".numberLines", tidy=FALSE}
par2pTmb <- list(logSdP = 0, logSdO = 0,
             z = rep(0, length(dataTmb$y)+1))
```

By default all values of `par2pTmb` will be estimated, but we assume that we know the value of the state at time 0, $z_0 = 0$. To provide this information, we can create a named list that we will input for the argument `map` of the function `MakeADFun` below. The elements of `map` should have the same name and size as those in `par2pTmb`, but we only need to specify the elements that we want to manipulate, here `par2pTmb$z`. To fix a value, we set that value to `NA`. The values that are estimated should have a different entry and all values should be factors.

```{r tmbMapPrep, attr.source = ".numberLines"}
mapTmb <- list(z=as.factor(c(NA,1:length(dataTmb$y))))
```

Before we can fit the model, we need to use the function `MakeADFun` to combine the data, the parameters, and the model and create a function that calculates the negative log likelihood and the gradients. 

To identify our random effects, namely the states, $z_t$, we set the argument `random` to equal the name of the parameters that are random effects.

```{r MakeADFun, attr.source = ".numberLines", tidy=FALSE}
m2pTmb <- RTMB::MakeADFun(toy2p, 
                    parameters = par2pTmb, 
                    map = mapTmb, 
                    random = "z",
                    silent = TRUE)

```

### Fitting the model to data

We fit the model using `nlminb`, which is a base R optimizer, but we input the object returned by `MakeADFun`.

```{r tmbOpt, attr.source = ".numberLines", tidy=FALSE, cache=TRUE}
f2pTmb <- nlminb(start = m2pTmb$par, # Initial values for the parameters
                 objective = m2pTmb$fn, # Function to be minimized
                 gradient = m2pTmb$gr) # Gradient of the objective
```

It is important to check whether the model converged.

```{r, attr.source = ".numberLines"}
f2pTmb$message
```

A message stating `"both X-convergence and relative convergence (5)"` would also indicate convergence.

### Exploring the results

To look at the parameter estimates, you can use the function `sdreport` and object object created by `MakeADFun`.

```{r tmbEst, attr.source = ".numberLines", tidy=FALSE}
sdr2pTmb <- summary(sdreport(m2pTmb))
# This will have the parameters and states
# So we can just print the parameters
round(cbind("Simulated" = c(sdp, sdo),
  sdr2pTmb[c("sdp", "sdo"),]),3)
```

We can see that the estimates for both the process variation and the observation error are close to their true value of 0.1, with relatively small standard errors (SEs).

We can get the smoothed state values with the `sdreport` function as above and this will give you the SEs for these states. If you are only interested in the state values (not theis SEs), you can also extract them directly from the model object.

```{r toy.states.est, attr.source = ".numberLines", tidy=FALSE, cache=TRUE}
# To get the point estimate and the SE of the states
zsSe2pTmb <- sdr2pTmb[row.names(sdr2pTmb)=="z",]
head(zsSe2pTmb)
# To get only the point estimates of the states
zs2pTmb <- m2pTmb$env$parList()$z
head(zs2pTmb)
```

Note that in this case, because we fixed the value of the state at time $t=0$, the first method, which looks at the predicted values, only returns state values for $t>0$.

We can use the SEs to calculate the 95\% confidence intervals.

```{r tmbCI, attr.source = ".numberLines", tidy=FALSE}
zsCIl2pTmb <- zsSe2pTmb[,1] + 
  qnorm(0.025, sd = zsSe2pTmb[,2])
zsCIu2pTmb <- zsSe2pTmb[,1] + 
  qnorm(0.975, sd = zsSe2pTmb[,2])
```

We can now overlay the state estimates along with their confidence intervals on the plot of the simulated data.

```{R tmbRp, attr.source = ".numberLines", tidy=FALSE}
plot(1:TT, y,
     pch=3, cex=0.8, col="blue", ty="o", lty = 3,
     xlab = "t", ylab = expression(z[t]),
     xlim = c(0,TT), ylim = c(min(y), max(y)+max(y)/5),
     las = 1)
points(0:TT, z,
       pch=19, cex=0.7, col = "red", ty="o")
polygon(c(1:TT, TT:1), c(zsCIl2pTmb,rev(zsCIu2pTmb)),
        col=rgb(1,0.7,0.4,0.3), border=FALSE)
lines(0:TT, zs2pTmb, 
      col= "darkgoldenrod1", lwd = 2)
legend("top",
       legend = c("Obs.", "True states", "Smooth. states"),
       pch = c(3, 19, NA),
       col = c("blue", "red", "darkgoldenrod1"),
       lwd = c(1, 1, 2), lty = c(3, 1, 1),
       horiz=TRUE, bty="n", cex=0.9)
```


# Fitting a movement state-space model with `RTMB`.

# State-space model from Argos movement data: a polar bear example

Using the data set available through Dryad Auger-Méthé and Derocher (2021) doi:10.5061/dryad.4qrfj6q96, we will explore a classic state-space model for Argos movement data, which is highly error prone. 

Let us load the Argos data.

```{r polar.bear.data, attr.source = ".numberLines"}
# Read the data
dataPbA <- read.csv("Data/PB_Argos.csv")
# Look at what it contains
head(dataPbA)
```

We can see that we have a column with the date and time, one with the Argos quality class, one with the latitude and one with the longitude.

Let us plot the data.

```{r polar.bear.plot, attr.source = ".numberLines", tidy=FALSE}
plot(dataPbA[,c("Lon", "Lat")], 
     pch=3, cex=0.7, col = "blue", ty="o", lty=3, las=1)
```

There are clear outliers, which are likely the results of observation error rather than a bear moving at record speed.

In Appendix S1 of Auger-Méthé et al. 2021, you can see how this movement compares to the GPS tracks.

Here we will use the Argos data to estimate states at regular time intervals. We will estimate states once a day at 17:00. The following code chunk is just data manipulation to create the $i$ index and the time proportion $j_i$ so that we relate the time of the observed Argos locations to these fix time intervals.

```{r time.interval, attr.source = ".numberLines", tidy=FALSE}
# Transform DateTime in R date.time class
dataPbA$Time <- as.POSIXlt(dataPbA$DateTime,
                       format="%Y.%m.%d %H:%M:%S", tz="UTC")
dataPbA$Date <- as.Date(dataPbA$Time)

# Create a regular time series that will
# represent the time at which we will estimate the states (z)
# Note that because our last Argos location is after 17:00,
# we end time series one day after
allDays <- seq(dataPbA$Date[1],
               dataPbA$Date[length(dataPbA$Time)] + 1, 1)
allDays <- as.POSIXlt(paste(allDays, "17:00"), tz="UTC")

# Find in between which z (allDays) observation y fall
# +1 because the it's the proportion of time between t-1 and t, we link with t.
dataPbA$idx <- findInterval(dataPbA$Time,allDays) + 1
# Checking how far from the z_t y is
dataPbA$ji <- as.numeric(24-difftime(allDays[dataPbA$idx],
                                 dataPbA$Time, units="hours"))/24

# dataPbA$ji <- as.numeric(difftime(allDays[dataPbA$idx],
#                                  dataPbA$Time, units="hours"))/24
```

It is customary to use error data to help the estimation process. Here we create a matrix with the error values, these values are based on those used in the package `bsam` (Jonsen et al. 2005). We have done a few changes to the values used in `bsam`. In particular, the skewness of the t-distribution is undefined when the degrees of freedom are $\leq 3$. We have thus set any degrees of freedom values smaller or equal to 3 to 3.00001.

```{r Argos.error, attr.source = ".numberLines", tidy=FALSE}
ArgosC <- matrix(nrow=6, ncol=4)
colnames(ArgosC) <- c("TLon","NuLon","TLat","NuLat")
ArgosC[,"TLon"] <- c(0.037842190, 0.004565261, 0.019461776,
                     0.008117727, 0.002807138, 0.002608584)
ArgosC[,"TLat"] <- c(0.027369282, 0.004594551, 0.014462340,
                     0.004142703, 0.002344425, 0.001098409)
ArgosC[,"NuLon"] <- c(3.00001, 3.00001, 3.00001,
                      3.00001, 3.00001, 3.070609)
ArgosC[,"NuLat"] <- c(3.00001, 3.00001, 3.00001,
                      3.896554, 6.314726, 3.00001)
rownames(ArgosC) <- c("B", "A", "0",
                      "1", "2", "3")
```

Now we create new columns in the data set that indicate the standard deviation and the degrees of freedom to use with each location.

```{r polar.bear.Argos.Cat, attr.source = ".numberLines"}
dataPbA$ac <- ArgosC[as.character(dataPbA$QualClass),]
```

## Fitting the model and checking model fit

### Create the function defining the negative log-likelihood

Again, the first thing we need to do to be able to fit the model in `RTMB` is to create the function that evaluates the negative log-likelihood value of the model.

```{r dcrw_nll, attr.source = ".numberLines", tidy=FALSE}
dcrw <- function(parms){
  # Make items within dataPbTmb and parms easily available
  getAll(dataPbTmb, parms, warn=FALSE)
  # Tell RTMB y is the response variable
  y <- OBS(y)
  
  # Transformation of the input parameters to model format
  # These transformations are made to insured that
  # the parameters have sensical values.
  # They do not change the model, 
  # they are only a computational trick.
  gamma <- 1.0/(1.0+exp(-logitGamma)) # b/c we want 0 < gamma < 1
  sdLon <- exp(logSdLon) # b/c we want sd > 0
  sdLat <- exp(logSdLat) # b/c we want sd > 0
  psiLon <- exp(logPsiLon) # b/c we want psi > 0
  psiLat <- exp(logPsiLat) # b/c we want psi > 0
  
  # Initializing the model
  # For the 2nd time step, we assume that we have a simple random walk:
  # z_1 = z_0 + eta
  # We are assuming independence between coordinates
  # Thus using two univariate normals
  # Start with lon, lat
  nll <- sum(dnorm(z[2,], mean = z[1,], sd = c(sdLon, sdLat), log=TRUE))
  
  # nll contribution of the process equation after the 2nd time step
  for(i in 3:nrow(z)){
    nll <- nll - sum(
      dnorm(z[i,], 
            mean = z[i-1,] + gamma*(z[i-1,] - z[i-2,]), 
            sd = c(sdLon, sdLat), 
            log = TRUE))
  }
  
  # nll contribution of the observation equation
  # The loop here is just the size of the observation vector.
  # We use the time index found in idx to relate
  # the appropriate state to the observation.
  for(i in 1:nrow(y)){
    # We are using the non-standardised t distribution,
    # that is why we are correcting the pdf.
    # Interpolate the value at time of observation
    tmp <- y[i,] -
      ((1-ji[i])*z[idx[i]-1,] +
      ji[i]*z[idx[i],])
    
    nll <- nll - sum(
      log(1/(c(psiLon, psiLat)*ac[i,c(1,3)])) +
      dt(tmp/(c(psiLon, psiLat)*ac[i,c(1,3)]),
         df = ac[i,c(2,4)],
         log = TRUE))
  }
    
  # Report the parameters and their standard errors in their model format
  ADREPORT(gamma)
  ADREPORT(sdLon)
  ADREPORT(sdLat)
  ADREPORT(psiLon)
  ADREPORT(psiLat)
  
  return(nll)
}

```



### Prepare data and set strating values for parameters

Then, we need to prepare the data, by which we mean create one list that contains all of the data elements needed by the functions. Here we need: 1) the matrix `y`, which contains the longitude and latitude of the locations; 2) the vector `idx`, which contains the index that relates the observation at time $i$ to the appropriate state time $t$; 3) the vector `ji`, which contains when the observation falls between $t-1$ and $t$; and 4) the matrix `ac`, which contains all of the error information for the given location.

```{r polar.bear.prep, attr.source = ".numberLines", tidy=FALSE}
dataPbTmb <- list(y = cbind(dataPbA$Lon, dataPbA$Lat),
               idx=dataPbA$idx, ji=dataPbA$ji, ac=dataPbA$ac)
```

We then need to create a list with the starting values for the parameters, including the states. To help the optimization, we will set the initial value of the states to be the values of the observed Argos location. This will just start the optimization at the good relative magnitude. We could have alternatively, scaled the model so to have transformed longitude and latitude values that are centered at 0.

```{r polar.bear.initial.val, attr.source = ".numberLines", tidy=FALSE}
# Setting the initial values for the state.
# We use the 1st obs. location to start the optimization
# at a relatively good place
zinitPbTmb <- matrix(as.numeric(dataPbA[1,c("Lon","Lat")]), 
                     max(dataPbA$idx), 2, byrow=TRUE)
# Creating a list of initial values for the parameters
parPbTmb <- list(logitGamma=0,
               logSdLon=0, logSdLat=0, logPsiLon=0, logPsiLat=0,
               z=zinitPbTmb)
```

### Fitting the state-space model to data

We have now all of the pieces needed to create the object with `MakeADFun`, which will allow us to minimize the negative log likelihood. Note that for more complex model, it sometimes help to allow for more iterations in the inner maximization. This can slow down the process, but here if we use the default maximum number of inner iterations, we will get the error. Thus, we increase the number of inner iterations to 5000 using `inner.control`.

```{r polar.bear.MakeADFun, attr.source = ".numberLines"}
mPbTmb <- MakeADFun(dcrw, parameters = parPbTmb, 
                    random = "z",
                    inner.control = list(maxit=5000),
                    silent = TRUE)
```

Using the created object, we can find the MLE with `nlminb`.

```{r polar.bear.nlminb, attr.source = ".numberLines", cache=TRUE}
fPbTmb <- nlminb(mPbTmb$par, mPbTmb$fn, mPbTmb$gr)
fPbTmb$message
```

Looks like it converged, so let us now look at the estimated parameters using `sdreport`.

```{r pbTMBpar, attr.source = ".numberLines", cache=TRUE}
parestPbTmb <- summary(sdreport(mPbTmb))
parestPbTmb[c("gamma", "sdLon", "sdLat", "psiLon", "psiLat"),]
```

So it appears that the movement of this bear is only partially dependent on the movement from the previous step, i.e., $\gamma =$ `r round(parestPbTmb["gamma",1],3)`.
Interestingly, the process variation is quite different in the latitude and longitude direction.
This could be due to the fact that one degree of latitude and one degree of longitude are not necessarily covering equivalent distance.
We see also that the correction factors for the longitude and latitude are quite different from one another and might point to the advantage of estimating one for each coordinate.

Now let us look at the state estimates. To extract the state value itself, we use `$env$parList()`. To extract the standard error, we use the matrix returned by `summary(sdreport())` above.

```{r polar.bear.states, attr.source = ".numberLines"}
# Get the point value
zsPbTmb <- mPbTmb$env$parList()$z
# Get the standard errors
zsvPbTmb <- parestPbTmb[rownames(parestPbTmb)%in%"z",]
zsvPbTmb <- cbind(zsvPbTmb[1:(nrow(zsvPbTmb)/2),2], 
                  zsvPbTmb[(1+nrow(zsvPbTmb)/2):nrow(zsvPbTmb),2])
```

Let's plot the results.

```{r polar.bear.plot.states, attr.source = ".numberLines", cache=TRUE}
plot(zsPbTmb, 
     pch=19, cex=0.7, col = "red", ty="o", las=1,
     xlab="Longitude", ylab="Latitude")
points(dataPbA[,c("Lon", "Lat")], 
       pch=3, cex=0.7, col = "blue", ty="o", lty=3)
legend("top",
       legend = c("Sim. Obs.", "Estimated latent states"),
       pch = c(3, 19),
       col = c("blue", "red"),
       lty = c(3, 1),
       horiz=TRUE, bty="n", cex=0.9)
```

In Appendix S1 of Auger-Méthé et al. 2021, we compare the estimated locations with this model to the GPS data, and see that the are quite similar.

# Literature cited

Auger-Méthé M, Derocher AE (2021) Argos and GPS data for a polar bear track, Dryad, Dataset, https://doi.org/10.5061/dryad.4qrfj6q96

Auger-Méthé M, Newman K, Cole D, Empacher F, Gryba R, King AA, Leos-Barajas V, Mills Flemming J, Nielsen A, Petris G, Thomas L (2021) A guide to state–space modeling of ecological time series. Ecological Monographs 91(4):e01470.  https://doi.org/10.1002/ecm.1470

Jonsen ID, Mills Flemming J, Myers RA (2005) Robust state-space modeling of animal movement data. Ecology 86:2874-2880
